import requests
import time
import json
import os
from typing import Dict, List, Tuple
from pathlib import Path

class OpenRouterTester:
    def __init__(self, config_file: str = "config.json"):
        self.config_file = config_file
        self.api_key = self._load_api_key()
        self.base_url = "https://openrouter.ai/api/v1/chat/completions"
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://github.com/your-username/ai-training",  # Опционально
            "X-Title": "AI Models Comparison"  # Опционально
        }
    
    def _load_api_key(self) -> str:
        """Загрузка API ключа из конфигурационного файла"""
        config_path = Path(self.config_file)
        
        if not config_path.exists():
            print(f"Конфигурационный файл '{self.config_file}' не найден.")
            print("Создайте файл config.json с содержимым:")
            print('''
{
    "openrouter_api_key": "ваш_api_ключ_здесь"
}
''')
            
            # Попробуем получить из переменных окружения
            env_key = os.getenv("OPENROUTER_API_KEY")
            if env_key:
                print("Используем ключ из переменной окружения OPENROUTER_API_KEY")
                return env_key
            
            raise ValueError(f"Не найден API ключ. Создайте файл {self.config_file}")
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                api_key = config.get("openrouter_api_key")
                
                if not api_key:
                    raise ValueError("Ключ 'openrouter_api_key' не найден в конфигурационном файле")
                
                return api_key
        except json.JSONDecodeError:
            raise ValueError(f"Ошибка чтения файла {self.config_file}. Проверьте формат JSON")
    
    def save_config_template(self):
        """Создание шаблона конфигурационного файла"""
        config_template = {
            "openrouter_api_key": "your_api_key_here",
            "models": {
                "small": "meta-llama/llama-3.2-3b-instruct",
                "medium": "google/gemma-3-12b-it",
                "large": "nousresearch/hermes-3-405b"
            },
            "test_prompts": [
                {
                    "name": "Пример промпта",
                    "prompt": "Объясни простыми словами, что такое ИИ"
                }
            ]
        }
        
        with open("config_template.json", "w", encoding="utf-8") as f:
            json.dump(config_template, f, ensure_ascii=False, indent=2)
        print("Создан файл config_template.json. Переименуйте его в config.json и добавьте свой API ключ")
    
    def query_model(self, model: str, prompt: str, system_prompt: str = None, max_tokens: int = 1000) -> Dict:
        """Отправка запроса к модели"""
        messages = []
        
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        messages.append({"role": "user", "content": prompt})
        
        payload = {
            "model": model,
            "messages": messages,
            "temperature": 0.7,
            "max_tokens": max_tokens,
            "stream": False  # Для упрощения подсчета токенов
        }
        
        try:
            start_time = time.perf_counter()
            response = requests.post(
                self.base_url,
                headers=self.headers,
                data=json.dumps(payload),
                timeout=60  # Таймаут 60 секунд
            )
            end_time = time.perf_counter()
            
            if response.status_code == 200:
                result = response.json()
                response_time = end_time - start_time
                
                # Получаем точное количество токенов из ответа
                usage = result.get("usage", {})
                input_tokens = usage.get("prompt_tokens", 0)
                output_tokens = usage.get("completion_tokens", 0)
                total_tokens = usage.get("total_tokens", 0)
                
                return {
                    "success": True,
                    "response": result["choices"][0]["message"]["content"],
                    "response_time": response_time,
                    "input_tokens": input_tokens,
                    "output_tokens": output_tokens,
                    "total_tokens": total_tokens,
                    "cost": 0,  # Бесплатные модели
                    "model": model,
                    "prompt": prompt,
                    "full_response": result
                }
            else:
                error_msg = f"HTTP {response.status_code}: {response.text}"
                return {
                    "success": False,
                    "error": error_msg,
                    "response_time": end_time - start_time,
                    "model": model
                }
                
        except requests.exceptions.Timeout:
            return {
                "success": False,
                "error": "Таймаут запроса (60 секунд)",
                "response_time": 60,
                "model": model
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "response_time": 0,
                "model": model
            }
    
    def compare_models(self, models: List[str], prompts: List[Dict], delay: float = 1.0) -> Dict:
        """Сравнение нескольких моделей на нескольких промптах"""
        results = {}
        
        for model_idx, model in enumerate(models):
            print(f"\n{'='*60}")
            print(f"Тестируем модель {model_idx+1}/{len(models)}: {model}")
            print('='*60)
            
            model_results = []
            total_time = 0
            total_tokens = 0
            successful_queries = 0
            
            for i, prompt_data in enumerate(prompts):
                print(f"\n[{i+1}/{len(prompts)}] {prompt_data['name']}")
                print(f"Вопрос: {prompt_data['prompt'][:80]}...")
                
                result = self.query_model(
                    model=model,
                    prompt=prompt_data["prompt"],
                    system_prompt=prompt_data.get("system", ""),
                    max_tokens=prompt_data.get("max_tokens", 1000)
                )
                
                if result["success"]:
                    model_results.append(result)
                    total_time += result["response_time"]
                    total_tokens += result["total_tokens"]
                    successful_queries += 1
                    
                    print(f"✓ Время: {result['response_time']:.2f} сек")
                    print(f"✓ Токены: {result['total_tokens']} (вход: {result['input_tokens']}, выход: {result['output_tokens']})")
                    print(f"✓ Ответ: {result['response'][:150]}...")
                else:
                    print(f"✗ Ошибка: {result['error']}")
                    model_results.append(result)
                
                # Небольшая задержка между запросами
                if i < len(prompts) - 1:
                    time.sleep(delay)
            
            results[model] = {
                "results": model_results,
                "total_time": total_time,
                "avg_time": total_time / successful_queries if successful_queries > 0 else 0,
                "total_tokens": total_tokens,
                "successful_queries": successful_queries,
                "total_queries": len(prompts)
            }
            
            # Задержка между моделями
            if model_idx < len(models) - 1:
                time.sleep(delay * 2)
        
        return results
    
    def print_comparison_report(self, results: Dict, prompts: List[Dict]):
        """Вывод сравнительного отчета"""
        print("\n" + "="*80)
        print("СРАВНИТЕЛЬНЫЙ ОТЧЕТ")
        print("="*80)
        
        # Статистика по моделям
        print(f"\n{'='*60}")
        print("ОБЩАЯ СТАТИСТИКА")
        print('='*60)
        
        print(f"\n{'Модель':<45} {'Успешно':<10} {'Ср. время':<12} {'Токены':<15} {'Токен/сек':<10}")
        print("-"*100)
        
        for model, stats in results.items():
            model_name_short = model.split('/')[-1][:40]
            tokens_per_sec = stats['total_tokens'] / stats['total_time'] if stats['total_time'] > 0 else 0
            print(f"{model_name_short:<45} {stats['successful_queries']}/{stats['total_queries']:<10} "
                  f"{stats['avg_time']:<12.2f} {stats['total_tokens']:<15} {tokens_per_sec:<10.2f}")
        
        # Детальное сравнение по каждому промпту
        for i, prompt_data in enumerate(prompts):
            print(f"\n{'='*60}")
            print(f"ПРОМПТ {i+1}: {prompt_data['name']}")
            print(f"Вопрос: {prompt_data['prompt']}")
            print('='*60)
            
            for model in results.keys():
                if (results[model]["results"] and 
                    i < len(results[model]["results"]) and 
                    results[model]["results"][i]["success"]):
                    
                    result = results[model]["results"][i]
                    print(f"\n--- {model.split('/')[-1]} ---")
                    print(f"Время: {result['response_time']:.2f} сек")
                    print(f"Токены: {result['total_tokens']} (вход: {result['input_tokens']}, выход: {result['output_tokens']})")
                    print(f"Ответ:\n{result['response']}\n")
        
        # Итоговый вывод
        print("\n" + "="*80)
        print("ИТОГОВЫЕ ВЫВОДЫ")
        print("="*80)
        
        # Находим лучшую модель по разным критериям
        fastest_model = min(results.items(), key=lambda x: x[1]['avg_time'])[0]
        most_efficient_model = max(results.items(), 
                                  key=lambda x: x[1]['total_tokens'] / x[1]['total_time'] 
                                  if x[1]['total_time'] > 0 else 0)[0]
        
        print(f"\nСамая быстрая модель: {fastest_model}")
        print(f"Самая эффективная (токенов/сек): {most_efficient_model}")
        
        # Рекомендации
        print("\nРекомендации:")
        print("1. Для быстрых ответов - используйте меньшие модели")
        print("2. Для сложных задач - используйте крупные модели")
        print("3. Бесплатные модели OpenRouter отлично подходят для экспериментов")


def load_prompts_from_config(config_file: str = "prompts.json"):
    """Загрузка промптов из конфигурационного файла"""
    default_prompts = [
        {
            "name": "Математическая задача",
            "prompt": "Реши задачу: У Васи было 5 яблок. Он отдал 2 яблока Пете и купил еще 4 яблока. Сколько яблок стало у Васи? Объясни свое решение.",
            "max_tokens": 300
        },
        {
            "name": "Логическая задача",
            "prompt": "Что тяжелее: килограмм пуха или килограмм железа? Объясни почему.",
            "max_tokens": 200
        },
        {
            "name": "Творческое задание",
            "prompt": "Напиши короткий рассказ (3-4 предложения) о роботе, который мечтает стать художником.",
            "max_tokens": 300
        },
        {
            "name": "Программирование",
            "prompt": "Напиши функцию на Python, которая проверяет, является ли строка палиндромом. Прокомментируй код.",
            "max_tokens": 400
        },
        {
            "name": "Объяснение концепции",
            "prompt": "Объясни разницу между машинным обучением и глубоким обучением простыми словами.",
            "max_tokens": 400
        }
    ]
    
    config_path = Path(config_file)
    if config_path.exists():
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                return config.get("prompts", default_prompts)
        except:
            print(f"Ошибка чтения {config_file}, использую стандартные промпты")
    
    return default_prompts


def main():
    """Основная функция"""
    print("="*80)
    print("СРАВНЕНИЕ AI МОДЕЛЕЙ НА OPENROUTER")
    print("="*80)
    
    # Инициализация тестера
    try:
        tester = OpenRouterTester("config.json")
    except ValueError as e:
        print(f"Ошибка: {e}")
        print("\nХотите создать шаблон конфигурационного файла? (y/n)")
        if input().lower() == 'y':
            tester = OpenRouterTester()
            tester.save_config_template()
        return
    
    # Выбранные модели для сравнения
    MODELS = [
        "meta-llama/llama-3.2-3b-instruct",  # Маленькая модель
        "google/gemma-3-12b-it",  # Средняя модель
        "nousresearch/hermes-3-405b"  # Крупная модель
    ]
    
    # Загрузка промптов
    prompts = load_prompts_from_config("prompts.json")
    
    print(f"\nБудут протестированы {len(MODELS)} модели на {len(prompts)} промптах")
    print("Модели:")
    for i, model in enumerate(MODELS, 1):
        print(f"  {i}. {model}")
    
    print("\nНачинаю тестирование...")
    
    # Запускаем сравнение
    results = tester.compare_models(MODELS, prompts, delay=0.5)
    
    # Выводим отчет
    tester.print_comparison_report(results, prompts)
    
    # Сохраняем результаты
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    output_file = f"results/comparison_results_{timestamp}.json"
    
    # Создаем папку для результатов если её нет
    Path("results").mkdir(exist_ok=True)
    
    # Подготавливаем данные для сохранения
    save_data = {
        "timestamp": timestamp,
        "models": MODELS,
        "prompts": prompts,
        "results": results
    }
    
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(save_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nПодробные результаты сохранены в: {output_file}")
    
    # Создаем краткий отчет в txt
    txt_report = f"results/summary_{timestamp}.txt"
    with open(txt_report, "w", encoding="utf-8") as f:
        f.write("СРАВНЕНИЕ AI МОДЕЛЕЙ\n")
        f.write("="*50 + "\n\n")
        
        for model, stats in results.items():
            f.write(f"Модель: {model}\n")
            f.write(f"  Успешных запросов: {stats['successful_queries']}/{stats['total_queries']}\n")
            f.write(f"  Среднее время: {stats['avg_time']:.2f} сек\n")
            f.write(f"  Всего токенов: {stats['total_tokens']}\n\n")
    
    print(f"Краткий отчет: {txt_report}")


if __name__ == "__main__":
    main()